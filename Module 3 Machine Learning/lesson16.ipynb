{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor,BaggingRegressor,ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression,Lasso,Ridge,ElasticNet\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,KFold,StratifiedKFold,GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder,Imputer,OneHotEncoder,MinMaxScaler, RobustScaler, StandardScaler,FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.stats import skew,randint\n",
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,f1_score,classification_report,roc_curve,roc_auc_score\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('../datasets/iowa_housing/train.csv', index_col='Id') \n",
    "X_test = pd.read_csv('../datasets/iowa_housing/test.csv', index_col='Id')\n",
    "y = X['SalePrice']\n",
    "X.drop('SalePrice',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_num_and_cat(df):\n",
    "        cat_cols = [col for col in df.columns if df[col].dtype == \"object\"]\n",
    "        num_cols = [col for col in df.columns if df[col].dtype in ['int64', 'float64']]\n",
    "        return cat_cols,num_cols\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols,num_cols = split_num_and_cat(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_count_cols = [col for col in cat_cols if X[col].nunique() < 10]\n",
    "high_count_cols = list(set(cat_cols)-set(low_count_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cat_cols),len(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(low_count_cols),len(high_count_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_values_percentage(df):\n",
    "    missing_values_counts_list = df.isnull().sum()\n",
    "    total_values = np.product(df.shape)\n",
    "    total_missing = missing_values_counts_list.sum()\n",
    "    # percent of data that is missing\n",
    "    return (total_missing/total_values) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " class myImputer(BaseEstimator, TransformerMixin):\n",
    "        def __init__(self,strategy='median'):\n",
    "            print('constructor with strategy {}'.format(strategy))\n",
    "            self.strategy = strategy\n",
    "        def fit(self, X, y=None):\n",
    "            print('fit called')\n",
    "            self.y = y\n",
    "            return self\n",
    "        def transform(self, X):\n",
    "            X = X.replace(0, np.NaN)\n",
    "            cols = X.columns\n",
    "            print('transform called')\n",
    "            if self.strategy == 'median':\n",
    "                X = X.fillna(X.median())\n",
    "            elif self.strategy == 'most_frequent':\n",
    "                for col in cols:\n",
    "                    X[col] = X[col].astype('category').cat.codes\n",
    "                X = X.fillna(X.mode())\n",
    "            \n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iowaFeatureEngineering(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,X,y):\n",
    "        return self\n",
    "    def transform(self,df):\n",
    "        df['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n",
    "        df[\"OverallGrade\"] = df[\"OverallQual\"] * df[\"OverallCond\"]\n",
    "        df['TotalLivArea'] = df['GrLivArea'] + df['GarageArea'] + df['LotArea']\n",
    "\n",
    "        df[\"GrLivArea-2\"] = df[\"GrLivArea\"] ** 2\n",
    "        df[\"GrLivArea-3\"] = df[\"GrLivArea\"] ** 3\n",
    "        df[\"GrLivArea-Sq\"] = np.sqrt(df[\"GrLivArea\"])\n",
    "        df[\"GarageArea-2\"] = df[\"GarageArea\"] ** 2\n",
    "        df[\"GarageArea-3\"] = df[\"GarageArea\"] ** 3\n",
    "        df[\"GarageArea-Sq\"] = np.sqrt(df[\"GarageArea\"])\n",
    "        return df\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_missing_values_percentage(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imp1 = myImputer(strategy='median')\n",
    "#df1 = X[num_cols]\n",
    "#imp1 = imp1.fit(df1)\n",
    "#df1 = imp1.transform(df1)\n",
    "#get_missing_values_percentage(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imp2 = myImputer(strategy='most_frequent')\n",
    "#df2 = X[num_cols]\n",
    "#imp2 = imp1.fit(df2)\n",
    "#df2 = imp1.transform(df2)\n",
    "#get_missing_values_percentage(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = pd.concat([df1,df2],axis=1).reset_index()\n",
    "#get_missing_values_percentage(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imp1 = myImputer(strategy=\"median\")\n",
    "#imp2 = myImputer(strategy=\"most_frequent\")\n",
    "#n1 = imp1.fit_transform(X[num_cols])\n",
    "#n2 = imp2.fit_transform(X[cat_cols])\n",
    "#n1.shape,n2.shape\n",
    "#X = pd.concat([n1,n2],axis=1).reset_index()\n",
    "#get_missing_values_percentage(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "        ('num_imputer', myImputer(strategy=\"median\")),\n",
    "        ('add_features',iowaFeatureEngineering()),\n",
    "        ('scaler', RobustScaler()),\n",
    "    ])\n",
    "cat_pipeline = Pipeline([\n",
    "        (\"cat_imputer\",myImputer(strategy=\"most_frequent\")),\n",
    "        #('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "   ])\n",
    "\n",
    "preprocessors = ColumnTransformer([\n",
    "        (\"num\",num_pipeline,num_cols),\n",
    "        (\"cat\",cat_pipeline,cat_cols),\n",
    "        \n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fe = iowaFeatureEngineering()\n",
    "#fe.fit(X,y)\n",
    "#X = fe.transform(X)\n",
    "#X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = preprocessors.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('../datasets/iowa_housing/train.csv', index_col='Id') \n",
    "X_test = pd.read_csv('../datasets/iowa_housing/test.csv', index_col='Id')\n",
    "y = X['SalePrice']\n",
    "X.drop('SalePrice',axis=1,inplace=True)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y,train_size=0.8,test_size=0.2,\n",
    "                                                      random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = preprocessors.fit_transform(X_train,y_train)\n",
    "X_valid = preprocessors.transform(X_valid)\n",
    "get_missing_values_percentage(pd.DataFrame(X_train))\n",
    "get_missing_values_percentage(pd.DataFrame(X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y,train_size=0.8,test_size=0.2,\n",
    "                                                      random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pipeline = Pipeline([ (\"preprocess\",preprocessors),\n",
    "                            (\"model\",GradientBoostingRegressor(n_estimators=100,random_state=100))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = final_pipeline.fit(X_train,y_train)\n",
    "preds = model.predict(X_valid)\n",
    "print('MAE:', mean_absolute_error(y_valid, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape,X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('../datasets/iowa_housing/train.csv', index_col='Id') \n",
    "X_test = pd.read_csv('../datasets/iowa_housing/test.csv', index_col='Id')\n",
    "y = X['SalePrice']\n",
    "X.drop('SalePrice',axis=1,inplace=True)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y,train_size=0.8,test_size=0.2,\n",
    "                                                      random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = preprocessors.fit_transform(X_train,y_train)\n",
    "X_valid = preprocessors.transform(X_valid)\n",
    "get_missing_values_percentage(pd.DataFrame(X_train))\n",
    "get_missing_values_percentage(pd.DataFrame(X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_estimators':[100,200,300,400,500,1500],\n",
    "           \"max_features\": randint(0,89),\n",
    "           \"min_samples_split\": randint(2, 11),\n",
    "           \"min_samples_leaf\": randint(1, 11),\n",
    "           \"subsample\":[0.6,0.7,0.75,0.8,0.9]\n",
    "         }\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "start = time()\n",
    "randomSearch_gb = RandomizedSearchCV(GradientBoostingRegressor(warm_start=True,random_state=100),\n",
    "                                     param_distributions=params,n_iter=20,\n",
    "                                     cv=kfold,n_jobs=6)        \n",
    "randomSearch_gb.fit(X_train,y_train)\n",
    "\n",
    "print('training took {} minutes'.format((time() - start)/60.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_dataset(model,X_train, X_valid, y_train, y_valid,error_fn=mean_absolute_error):\n",
    "    preds = model.predict(X_valid)\n",
    "    return error_fn(y_valid, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dataset(randomSearch_gb.best_estimator_,X_train, X_valid, y_train, y_valid,error_fn=mean_absolute_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(randomSearch_gb.best_estimator_,X_train,y_train,cv=4)\n",
    "print(\"Cross-validation scores: {}, mean score = {}\".format(scores,scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit Risk Assessment: A Classification Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_results(model,X_train,y_train,X_test,y_test,target_names=None):\n",
    "    model = model.fit(X_train, y_train)\n",
    "    print(\"Training set score: {:.3f}\".format(model.score(X_train, y_train)))\n",
    "    print(\"Test set score: {:.3f}\".format(model.score(X_test, y_test)))\n",
    "    preds = model.predict(X_test)\n",
    "    confusion = confusion_matrix(y_test, preds)\n",
    "    print(\"Confusion matrix:\\n{}\".format(confusion))\n",
    "    print('F1 score = {:.3f}'.format(f1_score(y_test, preds)))\n",
    "    print('ROC-AUC Score = {:.3f}'.format(roc_auc_score(y_test,preds)))\n",
    "    if target_names is not None:\n",
    "        print(classification_report(y_test, preds,target_names=target_names))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binaryRandomSampler(X,target,sample_type='under'):\n",
    "    count_class_0, count_class_1 = X[target].value_counts()\n",
    "    X_class_0 = X[X[target] == 0]\n",
    "    X_class_1 = X[X[target] == 1]\n",
    "    if count_class_0 < count_class_1:\n",
    "        X_lower_class = X_class_0\n",
    "        X_higher_class = X_class_1\n",
    "        count_class_lower = count_class_0\n",
    "        count_class_higher = count_class_1\n",
    "    else:\n",
    "        X_lower_class = X_class_1\n",
    "        X_higher_class = X_class_0\n",
    "        count_class_lower = count_class_1\n",
    "        count_class_higher = count_class_0\n",
    "        \n",
    "    if sample_type == 'under':\n",
    "        X_higher_class = X_higher_class.sample(count_class_lower)    \n",
    "    else:\n",
    "        X_lower_class = X_lower_class.sample(count_class_higher,replace=True)\n",
    "    \n",
    "    X = pd.concat([X_higher_class, X_lower_class], axis=0).reset_index()\n",
    "    X.drop('index',axis=1,inplace=True)\n",
    "\n",
    "    print('Random under-sampling:')\n",
    "    print(X[target].value_counts())\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('../datasets/give_me_credit/train.csv', low_memory=False,index_col=0)\n",
    "X_test = pd.read_csv('../datasets/give_me_credit/test.csv', low_memory=False,index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dropna(axis=0,inplace=True)\n",
    "get_missing_values_percentage(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['SeriousDlqin2yrs'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = binaryRandomSampler(X,'SeriousDlqin2yrs')\n",
    "y = X['SeriousDlqin2yrs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = binaryRandomSampler(X,'SeriousDlqin2yrs',sample_type='over')\n",
    "y = X['SeriousDlqin2yrs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X),len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(['SeriousDlqin2yrs'],axis=1,inplace=True)\n",
    "X_test.drop(['SeriousDlqin2yrs'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_cat(df,cols):\n",
    "    for col in cols:\n",
    "        df[col] = df[col].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_cat(X,['NumberOfTime30-59DaysPastDueNotWorse',\n",
    "                  'NumberOfTime60-89DaysPastDueNotWorse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y,stratify=y,\n",
    "                                                      train_size=0.8,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_pl_rf = Pipeline([ (\"scaler\",RobustScaler()),\n",
    "                            (\"model\",RandomForestClassifier(n_estimators=100,n_jobs=6,random_state=100))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_pl_gb = Pipeline([ (\"scaler\",RobustScaler()),\n",
    "                            (\"model\",GradientBoostingClassifier(n_estimators=100,random_state=100))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=100,warm_start=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100,n_jobs=6, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_classification_results(credit_pl_gb,X_train,y_train,X_valid,y_valid,target_names=[\"Low Risk\", \"High Risk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_classification_results(credit_pl_rf,X_train,y_train,X_valid,y_valid,target_names=[\"Low Risk\", \"High Risk\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
