{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor,BaggingRegressor,ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,KFold,StratifiedKFold,GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder,Imputer,OneHotEncoder,RobustScaler, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "from scipy.stats import randint\n",
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,f1_score,classification_report,roc_curve,roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit Risk Assessment: A Classification Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_values_percentage(df):\n",
    "    missing_values_counts_list = df.isnull().sum()\n",
    "    total_values = np.product(df.shape)\n",
    "    total_missing = missing_values_counts_list.sum()\n",
    "    # percent of data that is missing\n",
    "    return (total_missing/total_values) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_results(model,X_train,y_train,X_test,y_test,target_names=None):\n",
    "    model = model.fit(X_train, y_train)\n",
    "    print(\"Training set score: {:.3f}\".format(model.score(X_train, y_train)))\n",
    "    print(\"Test set score: {:.3f}\".format(model.score(X_test, y_test)))\n",
    "    preds = model.predict(X_test)\n",
    "    confusion = confusion_matrix(y_test, preds)\n",
    "    print(\"Confusion matrix:\\n{}\".format(confusion))\n",
    "    print('F1 score = {:.3f}'.format(f1_score(y_test, preds)))\n",
    "    print('ROC-AUC Score = {:.3f}'.format(roc_auc_score(y_test,preds)))\n",
    "    if target_names is not None:\n",
    "        print(classification_report(y_test, preds,target_names=target_names))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myImputer(BaseEstimator, TransformerMixin):\n",
    "        def __init__(self,strategy='median'):\n",
    "            print('constructor with strategy {}'.format(strategy))\n",
    "            self.strategy = strategy\n",
    "        def fit(self, X, y=None):\n",
    "            print('fit called')\n",
    "            self.y = y\n",
    "            return self\n",
    "        def transform(self, X):\n",
    "            X = X.replace(0, np.NaN)\n",
    "            cols = X.columns\n",
    "            print('transform called')\n",
    "            if self.strategy == 'median':\n",
    "                X = X.fillna(X.median())\n",
    "            elif self.strategy == 'most_frequent':\n",
    "                for col in cols:\n",
    "                    X[col] = X[col].astype('category').cat.codes\n",
    "                X = X.fillna(X.mode())\n",
    "            \n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binaryRandomSampler(X,target,sample_type='under'):\n",
    "    count_class_0, count_class_1 = X[target].value_counts()\n",
    "    X_class_0 = X[X[target] == 0]\n",
    "    X_class_1 = X[X[target] == 1]\n",
    "    if count_class_0 < count_class_1:\n",
    "        X_lower_class = X_class_0\n",
    "        X_higher_class = X_class_1\n",
    "        count_class_lower = count_class_0\n",
    "        count_class_higher = count_class_1\n",
    "    else:\n",
    "        X_lower_class = X_class_1\n",
    "        X_higher_class = X_class_0\n",
    "        count_class_lower = count_class_1\n",
    "        count_class_higher = count_class_0\n",
    "        \n",
    "    if sample_type == 'under':\n",
    "        X_higher_class = X_higher_class.sample(count_class_lower)    \n",
    "    else:\n",
    "        X_lower_class = X_lower_class.sample(count_class_higher,replace=True)\n",
    "    \n",
    "    X = pd.concat([X_higher_class, X_lower_class], axis=0).reset_index()\n",
    "    X.drop('index',axis=1,inplace=True)\n",
    "\n",
    "    print('Random under-sampling:')\n",
    "    print(X[target].value_counts())\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_over = Pipeline([ (\"scaler\",RobustScaler()),\n",
    "                      (\"model\",RandomForestClassifier(n_estimators=200,n_jobs=6,random_state=100))])\n",
    "rf_under = Pipeline([ (\"scaler\",RobustScaler()),\n",
    "                      (\"model\",RandomForestClassifier(n_estimators=200,n_jobs=6,random_state=100))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_over = Pipeline([ (\"scaler\",RobustScaler()),\n",
    "                     (\"model\",GradientBoostingClassifier(warm_start=True,n_estimators=200,random_state=100))])\n",
    "gb_under = Pipeline([ (\"scaler\",RobustScaler()),\n",
    "                     (\"model\",GradientBoostingClassifier(warm_start=True,n_estimators=200,random_state=100))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_over = Pipeline([ (\"scaler\",RobustScaler()),\n",
    "                     (\"model\",LogisticRegression(max_iter=10000,random_state=100))])\n",
    "logistic_under = Pipeline([ (\"scaler\",RobustScaler()),\n",
    "                     (\"model\",LogisticRegression(max_iter=10000,random_state=100))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('../datasets/give_me_credit/test.csv', low_memory=False,index_col=0)\n",
    "X_test.drop(['SeriousDlqin2yrs'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_path='../datasets/give_me_credit/train.csv',sample_type='under'):\n",
    "    X = pd.read_csv(train_path, low_memory=False,index_col=0)\n",
    "    X.dropna(axis=0,inplace=True)\n",
    "    if sample_type is not None:\n",
    "        X = binaryRandomSampler(X,'SeriousDlqin2yrs',sample_type=sample_type)\n",
    "    y = X['SeriousDlqin2yrs']\n",
    "    X.drop(['SeriousDlqin2yrs'],axis=1,inplace=True)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y,stratify=y,\n",
    "                                                          train_size=0.8,random_state=100)\n",
    "    return X_train,X_valid,y_train,y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_over, X_valid_over, y_train_over, y_valid_over = get_data(sample_type='over')\n",
    "X_train_under, X_valid_under, y_train_under, y_valid_under = get_data(sample_type='under')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_classification_results(rf_under,X_train_under,y_train_under,X_valid_under,y_valid_under,target_names=[\"Low Risk\", \"High Risk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_classification_results(rf_over,X_train_over,y_train_over,X_valid_over,y_valid_over,target_names=[\"Low Risk\", \"High Risk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_classification_results(gb_under,X_train_under,y_train_under,X_valid_under,y_valid_under,target_names=[\"Low Risk\", \"High Risk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_classification_results(gb_over,X_train_over,y_train_over,X_valid_over,y_valid_over,target_names=[\"Low Risk\", \"High Risk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Pipeline([ (\"scaler\",RobustScaler())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = preprocessor.fit_transform(X_train_under,y_train_under)\n",
    "X_valid = preprocessor.fit_transform(X_valid_under,y_valid_under)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = {'n_estimators':[100,200,300,500],\n",
    "           \"max_features\": randint(8,11),\n",
    "           \"min_samples_split\": randint(2, 11),\n",
    "           \"min_samples_leaf\": randint(1, 11),\n",
    "         }\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "start = time()\n",
    "randomSearch_rf = RandomizedSearchCV(RandomForestClassifier(warm_start=True),\n",
    "                                     param_distributions=params,n_iter=20,\n",
    "                                     cv=kfold,n_jobs=6)        \n",
    "randomSearch_rf.fit(X_train,y_train_under)\n",
    "\n",
    "print('training took {} minutes'.format((time() - start)/60.))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "params = {'n_estimators':[100,200,300,500],\n",
    "           \"max_features\": randint(8,11),\n",
    "           \"min_samples_split\": randint(2, 11),\n",
    "           \"min_samples_leaf\": randint(1, 11),\n",
    "         }\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "start = time()\n",
    "randomSearch_gb = RandomizedSearchCV(GradientBoostingClassifier(warm_start=True,random_state=100),\n",
    "                                     param_distributions=params,n_iter=20,\n",
    "                                     cv=kfold,n_jobs=6)        \n",
    "randomSearch_gb.fit(X_train,y_train_under)\n",
    "\n",
    "print('training took {} minutes'.format((time() - start)/60.))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_classification_results(randomSearch_rf.best_estimator_,X_train,y_train_under,X_valid,y_valid_under,target_names=[\"Low Risk\", \"High Risk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_classification_results(randomSearch_gb.best_estimator_,X_train,y_train,X_valid,y_valid,target_names=[\"Low Risk\", \"High Risk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_missing_values_percentage(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_over = [('RF_over',rf_over),\n",
    "           ('GB_over',gb_over),\n",
    "           ('logistic_over',logistic_over)\n",
    "         ]\n",
    "\n",
    "stacked_models_over = tuple([model[1] for model in models_over])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_under = [ ('RF_under',rf_under),\n",
    "           ('GB_under',gb_under),\n",
    "           ('logistic_under',logistic_under)\n",
    "         ]\n",
    "stacked_models_under = tuple([model[1] for model in models_under])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_gen_over = StackingCVClassifier(classifiers=stacked_models_over,\n",
    "                                meta_classifier= models_over[2][1],\n",
    "                                use_features_in_secondary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_gen_under = StackingCVClassifier(classifiers=stacked_models_under,\n",
    "                                meta_classifier= models_under[2][1],\n",
    "                                use_features_in_secondary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_classification_results(stack_gen_over,np.array(X_train_over), np.array(y_train_over),\n",
    "                           np.array(X_valid_over),y_valid_over,\n",
    "                           target_names=[\"Low Risk\", \"High Risk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_classification_results(stack_gen_under,np.array(X_train_under), \n",
    "                           np.array(y_train_under),np.array(X_valid_under),\n",
    "                           y_valid_under,target_names=[\"Low Risk\", \"High Risk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp1 = myImputer(strategy='median')\n",
    "df1 = X_test\n",
    "imp1 = imp1.fit(df1)\n",
    "df1 = imp1.transform(df1)\n",
    "get_missing_values_percentage(df1)\n",
    "preds_test = stack_gen_over.predict(df1)\n",
    "pd.DataFrame(preds_test,columns=['Probability']).Probability.value_counts()\n",
    "output = pd.DataFrame({'Id': X_test.index,\n",
    "                       'Probability': preds_test})\n",
    "output.to_csv('submission_credit_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
